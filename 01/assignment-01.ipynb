{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d99534af",
      "metadata": {
        "id": "d99534af",
        "outputId": "9df7e77c-57cc-4318-bb32-f0474a267bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sacremoses'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2522790337.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msacremoses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMosesTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMosesDetokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sacremoses'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import list_datasets\n",
        "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import pairwise\n",
        "\n",
        "dataset_name = \"ufal/npfl147\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aad9c5bf",
      "metadata": {
        "id": "aad9c5bf"
      },
      "outputs": [],
      "source": [
        "# dataset_cs = load_dataset(dataset_name, \"cs\")\n",
        "# dataset_en = load_dataset(dataset_name, \"en\")\n",
        "# dataset_sk = load_dataset(dataset_name, \"sk\")\n",
        "\n",
        "dataset = {\n",
        "    \"cs\": load_dataset(dataset_name, \"cs\"),\n",
        "    \"en\": load_dataset(dataset_name, \"en\"),\n",
        "    \"sk\": load_dataset(dataset_name, \"sk\"),\n",
        "}\n",
        "\n",
        "moses_tokenizer = {\n",
        "    \"cs\": MosesTokenizer(lang=\"cs\"),\n",
        "    \"en\": MosesTokenizer(lang=\"en\"),\n",
        "    \"sk\": MosesTokenizer(lang=\"sk\"),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b07ff810",
      "metadata": {
        "id": "b07ff810"
      },
      "outputs": [],
      "source": [
        "corpus = {\n",
        "    lang: \"\".join(dataset[lang][\"train\"][\"text\"]) for lang in dataset.keys()\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1eb243e",
      "metadata": {
        "id": "d1eb243e"
      },
      "outputs": [],
      "source": [
        "tokenized = {\n",
        "    lang: moses_tokenizer[lang].tokenize(corpus[lang]) for lang in dataset.keys()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cfe6b8d",
      "metadata": {
        "id": "0cfe6b8d",
        "outputId": "84cd7def-1a8f-420c-e2f0-1a6fd048d964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "955309\n"
          ]
        }
      ],
      "source": [
        "# English stats: Data size\n",
        "# How many tokens of English did you count after applying MosesTokenizer?\n",
        "\n",
        "print(len(tokenized[\"en\"])) # 955309"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa09966",
      "metadata": {
        "id": "daa09966",
        "outputId": "2bb42e5f-0f4d-477d-f1ce-fd30034c86ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "71970\n"
          ]
        }
      ],
      "source": [
        "# English stats: Unigrams\n",
        "# How many unique unigrams of English did you count after applying MosesTokenizer?\n",
        "\n",
        "print(len(set(tokenized[\"en\"]))) # 71970"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52df236f",
      "metadata": {
        "id": "52df236f"
      },
      "outputs": [],
      "source": [
        "start_special_symbol = \"<s>\"\n",
        "end_special_symbol = \"</s>\"\n",
        "\n",
        "get_unigrams = lambda tokens : Counter(tokens)\n",
        "get_bigrams = lambda tokens : Counter(zip([start_special_symbol] + tokens, tokens + [end_special_symbol]))\n",
        "get_trigrams = lambda tokens : Counter(zip(2 * [start_special_symbol] + tokens, tokens + 2 * [end_special_symbol]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc079d7",
      "metadata": {
        "id": "4fc079d7",
        "outputId": "71f6ab46-250b-4d43-e673-bc92c7a011f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "376077\n"
          ]
        }
      ],
      "source": [
        "# English stats: Bigrams\n",
        "# How many unique bigrams of English did you count after applying MosesTokenizer?\n",
        "\n",
        "print(len(get_bigrams(tokenized[\"en\"]))) # 376077\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2385e00",
      "metadata": {
        "id": "f2385e00"
      },
      "outputs": [],
      "source": [
        "def entropy_bigrams(bi, uni):\n",
        "    N_bi, N_uni = sum(bi.values()), sum(uni.values())\n",
        "    uni[start_special_symbol] = uni[start_special_symbol] if start_special_symbol in uni else 1\n",
        "    uni[end_special_symbol] = uni[end_special_symbol] if end_special_symbol in uni else 1\n",
        "    H = 0.0\n",
        "    for (x, y), count in bi.items():\n",
        "        p_xy = count / N_bi\n",
        "        p_y = uni[y] / N_uni\n",
        "        H += - p_xy * np.log2(p_xy / p_y)\n",
        "    return H\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d23f1a8",
      "metadata": {
        "id": "3d23f1a8",
        "outputId": "a3da7093-43dd-41ca-b2fd-198b9f225b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English entropy:  5.67234587839017\n",
            "Czech entropy:  5.064022671529996\n",
            "Slovak entropy:  4.52334189845233\n"
          ]
        }
      ],
      "source": [
        "# English stats: Entropy\n",
        "# What is the conditional entropy of the English dataset tokenized with MosesTokenizer?\n",
        "\n",
        "print(\"English entropy: \", entropy_bigrams(get_bigrams(tokenized[\"en\"]), get_unigrams(tokenized[\"en\"])))\n",
        "\n",
        "# Czech stats: Entropy\n",
        "# What is the conditional entropy of the Czech dataset tokenized with MosesTokenizer?\n",
        "\n",
        "print(\"Czech entropy: \", entropy_bigrams(get_bigrams(tokenized[\"cs\"]), get_unigrams(tokenized[\"cs\"])))\n",
        "\n",
        "# Entropy of the other language\n",
        "# What is the conditional entropy of the dataset of the language you chose tokenized with MosesTokenizer? (If you chose more, pick one)\n",
        "\n",
        "print(\"Slovak entropy: \", entropy_bigrams(get_bigrams(tokenized[\"sk\"]), get_unigrams(tokenized[\"sk\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aadac687",
      "metadata": {
        "id": "aadac687",
        "outputId": "2df7d799-0f91-4b23-d136-ee064ef72931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    tokenizer       language      data_size    nr_unigrams    nr_bigrams    entropy_bigrams\n",
            "--  --------------  ----------  -----------  -------------  ------------  -----------------\n",
            " 0  MosesTokenizer  cs               596400          91024        337350            5.06402\n",
            " 0  MosesTokenizer  en               955309          71970        376077            5.67235\n",
            " 0  MosesTokenizer  sk               409435          65541        211080            4.52334\n"
          ]
        }
      ],
      "source": [
        "def detailed_stats_language(tokens, language_name=\"unknown\", tokenizer=\"unknown\"):\n",
        "    unigrams = get_unigrams(tokens)\n",
        "    bigrams = get_bigrams(tokens)\n",
        "    trigrams = get_trigrams(tokens)\n",
        "\n",
        "    stats = pd.DataFrame({\n",
        "        \"tokenizer\": [tokenizer],\n",
        "        \"language\": [language_name],\n",
        "        \"data_size\": [len(tokens)],\n",
        "        \"nr_unigrams\": [len(unigrams)],\n",
        "        \"nr_bigrams\": [len(bigrams)],\n",
        "        \"entropy_bigrams\": [entropy_bigrams(bigrams, unigrams)],\n",
        "    }\n",
        "    )\n",
        "    return stats\n",
        "\n",
        "all_stats = pd.concat([detailed_stats_language(tokens, language, tokenizer=\"MosesTokenizer\") for language, tokens in tokenized.items()])\n",
        "print(tabulate(all_stats, headers='keys')) # is there a way to drop the id column, that is implicit from pandas?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211a21bb",
      "metadata": {
        "id": "211a21bb",
        "outputId": "b3e79d75-d50c-4e8d-cb5f-f02fcef22c4d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# now i need to use xml-r tokenizer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# from transformers import AutoModelForMaskedLM, AutoTokenizer, BitsAndBytesConfig\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# xlm = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForMaskedLM, AutoTokenizer\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFacebookAI/xlm-roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFacebookAI/xlm-roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     15\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# now i need to use xml-r tokenizer\n",
        "# from transformers import AutoModelForMaskedLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# xlm = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"FacebookAI/xlm-roberta-base\"\n",
        ")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\n",
        "    \"FacebookAI/xlm-roberta-base\",\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"sdpa\"\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}