{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99534af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import list_datasets\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import pairwise\n",
    "\n",
    "dataset_name = \"ufal/npfl147\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_cs = load_dataset(dataset_name, \"cs\")\n",
    "# dataset_en = load_dataset(dataset_name, \"en\")\n",
    "# dataset_sk = load_dataset(dataset_name, \"sk\")\n",
    "\n",
    "dataset = {\n",
    "    \"cs\": load_dataset(dataset_name, \"cs\"),\n",
    "    \"en\": load_dataset(dataset_name, \"en\"),\n",
    "    \"sk\": load_dataset(dataset_name, \"sk\"),\n",
    "}\n",
    "\n",
    "moses_tokenizer = {\n",
    "    \"cs\": MosesTokenizer(lang=\"cs\"),\n",
    "    \"en\": MosesTokenizer(lang=\"en\"),\n",
    "    \"sk\": MosesTokenizer(lang=\"sk\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07ff810",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {\n",
    "    lang: \"\".join(dataset[lang][\"train\"][\"text\"]) for lang in dataset.keys()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1eb243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = {\n",
    "    lang: moses_tokenizer[lang].tokenize(corpus[lang]) for lang in dataset.keys()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cfe6b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "955309\n"
     ]
    }
   ],
   "source": [
    "# English stats: Data size\n",
    "# How many tokens of English did you count after applying MosesTokenizer?\n",
    "\n",
    "print(len(tokenized[\"en\"])) # 955309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daa09966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71970\n"
     ]
    }
   ],
   "source": [
    "# English stats: Unigrams\n",
    "# How many unique unigrams of English did you count after applying MosesTokenizer?\n",
    "\n",
    "print(len(set(tokenized[\"en\"]))) # 71970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_special_symbol = \"<s>\"\n",
    "end_special_symbol = \"</s>\"\n",
    "\n",
    "get_unigrams = lambda tokens : Counter(tokens)\n",
    "get_bigrams = lambda tokens : Counter(zip([start_special_symbol] + tokens, tokens + [end_special_symbol]))\n",
    "get_trigrams = lambda tokens : Counter(zip(2 * [start_special_symbol] + tokens, tokens + 2 * [end_special_symbol]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc079d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376077\n"
     ]
    }
   ],
   "source": [
    "# English stats: Bigrams\n",
    "# How many unique bigrams of English did you count after applying MosesTokenizer?\n",
    "\n",
    "print(len(get_bigrams(tokenized[\"en\"]))) # 376077\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2385e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_bigrams(bi, uni):\n",
    "    N_bi, N_uni = sum(bi.values()), sum(uni.values())\n",
    "    uni[start_special_symbol] = uni[start_special_symbol] if start_special_symbol in uni else 1\n",
    "    uni[end_special_symbol] = uni[end_special_symbol] if end_special_symbol in uni else 1\n",
    "    H = 0.0\n",
    "    for (x, y), count in bi.items():\n",
    "        p_xy = count / N_bi\n",
    "        p_y = uni[y] / N_uni\n",
    "        H += - p_xy * np.log2(p_xy / p_y)\n",
    "    return H\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d23f1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English entropy:  5.67234587839017\n",
      "Czech entropy:  5.064022671529996\n",
      "Slovak entropy:  4.52334189845233\n"
     ]
    }
   ],
   "source": [
    "# English stats: Entropy\n",
    "# What is the conditional entropy of the English dataset tokenized with MosesTokenizer?\n",
    "\n",
    "print(\"English entropy: \", entropy_bigrams(get_bigrams(tokenized[\"en\"]), get_unigrams(tokenized[\"en\"])))\n",
    "\n",
    "# Czech stats: Entropy\n",
    "# What is the conditional entropy of the Czech dataset tokenized with MosesTokenizer?\n",
    "\n",
    "print(\"Czech entropy: \", entropy_bigrams(get_bigrams(tokenized[\"cs\"]), get_unigrams(tokenized[\"cs\"])))\n",
    "\n",
    "# Entropy of the other language\n",
    "# What is the conditional entropy of the dataset of the language you chose tokenized with MosesTokenizer? (If you chose more, pick one)\n",
    "\n",
    "print(\"Slovak entropy: \", entropy_bigrams(get_bigrams(tokenized[\"sk\"]), get_unigrams(tokenized[\"sk\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadac687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    tokenizer       language      data_size    nr_unigrams    nr_bigrams    entropy_bigrams\n",
      "--  --------------  ----------  -----------  -------------  ------------  -----------------\n",
      " 0  MosesTokenizer  cs               596400          91024        337350            5.06402\n",
      " 0  MosesTokenizer  en               955309          71970        376077            5.67235\n",
      " 0  MosesTokenizer  sk               409435          65541        211080            4.52334\n"
     ]
    }
   ],
   "source": [
    "def detailed_stats_language(tokens, language_name=\"unknown\", tokenizer=\"unknown\"):\n",
    "    unigrams = get_unigrams(tokens)\n",
    "    bigrams = get_bigrams(tokens)\n",
    "    trigrams = get_trigrams(tokens)\n",
    "\n",
    "    stats = pd.DataFrame({\n",
    "        \"tokenizer\": [tokenizer],\n",
    "        \"language\": [language_name],\n",
    "        \"data_size\": [len(tokens)],\n",
    "        \"nr_unigrams\": [len(unigrams)],\n",
    "        \"nr_bigrams\": [len(bigrams)],\n",
    "        \"entropy_bigrams\": [entropy_bigrams(bigrams, unigrams)],\n",
    "    }\n",
    "    )\n",
    "    return stats\n",
    "\n",
    "all_stats = pd.concat([detailed_stats_language(tokens, language, tokenizer=\"MosesTokenizer\") for language, tokens in tokenized.items()])\n",
    "print(tabulate(all_stats, headers='keys')) # is there a way to drop the id column, that is implicit from pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "211a21bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# now i need to use xml-r tokenizer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# from transformers import AutoModelForMaskedLM, AutoTokenizer, BitsAndBytesConfig\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# xlm = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForMaskedLM, AutoTokenizer\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFacebookAI/xlm-roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFacebookAI/xlm-roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     15\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     attn_implementation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# now i need to use xml-r tokenizer\n",
    "# from transformers import AutoModelForMaskedLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# xlm = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"FacebookAI/xlm-roberta-base\"\n",
    ")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"FacebookAI/xlm-roberta-base\",\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
