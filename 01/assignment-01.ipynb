{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0661b259",
   "metadata": {},
   "source": [
    "In this assignment you will explore the entropy of natural language and n-gram language smoothing across multiple languages. Your task is to obtain a dataset from the Hugging Face repository and calculate conditional entropy of three languages. You will experiment with how the entropy changes with regard to the tokenization strategy (i.e. how you split text into sequential inputs for the model). Then, you will implement interpolated smoothing for trigram language models and use the EM algorithm to optimize smoothing parameters and evaluate model performance through cross-entropy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99534af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import list_datasets\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "dataset_name = \"ufal/npfl147\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad9c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cs = load_dataset(dataset_name, \"cs\")\n",
    "dataset_en = load_dataset(dataset_name, \"en\")\n",
    "dataset_sk = load_dataset(dataset_name, \"sk\")\n",
    "\n",
    "cs_tokenizer = MosesTokenizer(lang=\"cs\")\n",
    "en_tokenizer = MosesTokenizer(lang=\"en\")\n",
    "sk_tokenizer = MosesTokenizer(lang=\"sk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07ff810",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_corpus = \"\".join(dataset_cs[\"train\"][\"text\"])\n",
    "en_corpus = \"\".join(dataset_en[\"train\"][\"text\"])\n",
    "sk_corpus = \"\".join(dataset_sk[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1eb243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_tok = cs_tokenizer.tokenize(cs_corpus)\n",
    "en_tok = en_tokenizer.tokenize(en_corpus)\n",
    "sk_tok = sk_tokenizer.tokenize(sk_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe6b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "955309\n"
     ]
    }
   ],
   "source": [
    "# English stats: Data size\n",
    "# How many tokens of English did you count after applying MosesTokenizer?\n",
    "\n",
    "print(len(en_tok)) # 955309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa09966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71970\n"
     ]
    }
   ],
   "source": [
    "# English stats: Unigrams\n",
    "# How many unique unigrams of English did you count after applying MosesTokenizer?\n",
    "\n",
    "print(len(set(en_tok))) # 71970"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
