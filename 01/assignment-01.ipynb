{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0661b259",
   "metadata": {},
   "source": [
    "In this assignment you will explore the entropy of natural language and n-gram language smoothing across multiple languages. Your task is to obtain a dataset from the Hugging Face repository and calculate conditional entropy of three languages. You will experiment with how the entropy changes with regard to the tokenization strategy (i.e. how you split text into sequential inputs for the model). Then, you will implement interpolated smoothing for trigram language models and use the EM algorithm to optimize smoothing parameters and evaluate model performance through cross-entropy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d99534af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import list_datasets\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "dataset_name = \"ufal/npfl147\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad9c5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 7313.25 examples/s]\n",
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 14563.56 examples/s]\n",
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 29046.63 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'id', 'wikiname', 'page_id', 'title', 'url', 'date_modified', 'in_language', 'wikidata_id', 'bytes_html', 'wikitext', 'version', 'infoboxes', 'has_math'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'id', 'wikiname', 'page_id', 'title', 'url', 'date_modified', 'in_language', 'wikidata_id', 'bytes_html', 'wikitext', 'version', 'infoboxes', 'has_math'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'id', 'wikiname', 'page_id', 'title', 'url', 'date_modified', 'in_language', 'wikidata_id', 'bytes_html', 'wikitext', 'version', 'infoboxes', 'has_math'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_cs = load_dataset(dataset_name, \"cs\")\n",
    "dataset_en = load_dataset(dataset_name, \"en\")\n",
    "dataset_sk = load_dataset(dataset_name, \"sk\")\n",
    "print(dataset_cs)\n",
    "print(dataset_en)\n",
    "print(dataset_sk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
