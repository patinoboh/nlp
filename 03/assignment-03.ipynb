{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6bf5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conllu in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (6.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: stanza in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: tomli in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from stanza) (2.4.0)\n",
      "Requirement already satisfied: numpy in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from stanza) (2.2.6)\n",
      "Requirement already satisfied: requests in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from stanza) (2.32.5)\n",
      "Requirement already satisfied: tqdm in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from stanza) (2.9.1)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from stanza) (6.33.5)\n",
      "Requirement already satisfied: emoji in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from stanza) (2.15.0)\n",
      "Requirement already satisfied: networkx in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from stanza) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (3.1.6)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (12.8.90)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (4.15.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (12.5.8.93)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (1.14.0)\n",
      "Requirement already satisfied: filelock in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (3.20.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (3.3.20)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (0.7.1)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (2.27.5)\n",
      "Requirement already satisfied: triton==3.5.1 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (3.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from torch>=1.13.0->stanza) (12.8.90)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from requests->stanza) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from requests->stanza) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from requests->stanza) (2025.11.12)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from requests->stanza) (3.4.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/disk/patrik/nlp/.venv_nlp/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->stanza) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install conllu\n",
    "%pip install stanza\n",
    "\n",
    "import conllu\n",
    "import pandas as pd\n",
    "import stanza\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import pairwise\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f38ecd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-02-09 20:35:16--  https://raw.githubusercontent.com/UniversalDependencies/UD_Czech-PDTC/refs/heads/master/cs_pdtc-ud-train-la.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 24175776 (23M) [text/plain]\n",
      "Saving to: ‘cs_pdtc-ud-train-la.conllu’\n",
      "\n",
      "cs_pdtc-ud-train-la 100%[===================>]  23,06M  34,4MB/s    in 0,7s    \n",
      "\n",
      "2026-02-09 20:35:19 (34,4 MB/s) - ‘cs_pdtc-ud-train-la.conllu’ saved [24175776/24175776]\n",
      "\n",
      "--2026-02-09 20:35:19--  https://raw.githubusercontent.com/UniversalDependencies/UD_Czech-PDTC/refs/heads/master/cs_pdtc-ud-test.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 39742361 (38M) [text/plain]\n",
      "Saving to: ‘cs_pdtc-ud-test.conllu’\n",
      "\n",
      "cs_pdtc-ud-test.con 100%[===================>]  37,90M  4,90MB/s    in 3,8s    \n",
      "\n",
      "2026-02-09 20:35:24 (9,94 MB/s) - ‘cs_pdtc-ud-test.conllu’ saved [39742361/39742361]\n",
      "\n",
      "--2026-02-09 20:35:25--  https://raw.githubusercontent.com/UniversalDependencies/UD_Czech-PDTC/refs/heads/master/cs_pdtc-ud-dev.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50117913 (48M) [text/plain]\n",
      "Saving to: ‘cs_pdtc-ud-dev.conllu’\n",
      "\n",
      "cs_pdtc-ud-dev.conl 100%[===================>]  47,80M  29,6MB/s    in 1,6s    \n",
      "\n",
      "2026-02-09 20:35:28 (29,6 MB/s) - ‘cs_pdtc-ud-dev.conllu’ saved [50117913/50117913]\n",
      "\n",
      "--2026-02-09 20:35:28--  https://raw.githubusercontent.com/UniversalDependencies/UD_English-GUM/refs/heads/master/en_gum-ud-train.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18490556 (18M) [text/plain]\n",
      "Saving to: ‘en_gum-ud-train.conllu’\n",
      "\n",
      "en_gum-ud-train.con 100%[===================>]  17,63M  42,2MB/s    in 0,4s    \n",
      "\n",
      "2026-02-09 20:35:31 (42,2 MB/s) - ‘en_gum-ud-train.conllu’ saved [18490556/18490556]\n",
      "\n",
      "--2026-02-09 20:35:31--  https://raw.githubusercontent.com/UniversalDependencies/UD_English-GUM/refs/heads/master/en_gum-ud-test.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2926279 (2,8M) [text/plain]\n",
      "Saving to: ‘en_gum-ud-test.conllu’\n",
      "\n",
      "en_gum-ud-test.conl 100%[===================>]   2,79M  --.-KB/s    in 0,1s    \n",
      "\n",
      "2026-02-09 20:35:32 (23,6 MB/s) - ‘en_gum-ud-test.conllu’ saved [2926279/2926279]\n",
      "\n",
      "--2026-02-09 20:35:32--  https://raw.githubusercontent.com/UniversalDependencies/UD_English-GUM/refs/heads/master/en_gum-ud-dev.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2924380 (2,8M) [text/plain]\n",
      "Saving to: ‘en_gum-ud-dev.conllu’\n",
      "\n",
      "en_gum-ud-dev.conll 100%[===================>]   2,79M  --.-KB/s    in 0,1s    \n",
      "\n",
      "2026-02-09 20:35:32 (25,6 MB/s) - ‘en_gum-ud-dev.conllu’ saved [2924380/2924380]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_Czech-PDTC/refs/heads/master/cs_pdtc-ud-train-la.conllu -O cs_pdtc-ud-train-la.conllu\n",
    "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_Czech-PDTC/refs/heads/master/cs_pdtc-ud-test.conllu -O cs_pdtc-ud-test.conllu\n",
    "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_Czech-PDTC/refs/heads/master/cs_pdtc-ud-dev.conllu -O cs_pdtc-ud-dev.conllu\n",
    "\n",
    "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-GUM/refs/heads/master/en_gum-ud-train.conllu -O en_gum-ud-train.conllu\n",
    "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-GUM/refs/heads/master/en_gum-ud-test.conllu -O en_gum-ud-test.conllu\n",
    "!wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-GUM/refs/heads/master/en_gum-ud-dev.conllu -O en_gum-ud-dev.conllu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd83b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_test = {\n",
    "    \"cs\" : \"cs_pdtc-ud-test.conllu\",\n",
    "    \"en\": \"en_gum-ud-test.conllu\"\n",
    "}\n",
    "\n",
    "filenames_dev = {\n",
    "    \"cs\" : \"cs_pdtc-ud-dev.conllu\",\n",
    "    \"en\": \"en_gum-ud-dev.conllu\"\n",
    "}\n",
    "\n",
    "filenames_train = {\n",
    "    \"cs\" : \"cs_pdtc-ud-train-la.conllu\",\n",
    "    \"en\": \"en_gum-ud-train.conllu\"\n",
    "}\n",
    "\n",
    "filenames = {\n",
    "    \"test\": filenames_test,\n",
    "    \"dev\": filenames_dev,\n",
    "    \"train\": filenames_train\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d8629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dict()\n",
    "\n",
    "for name, d in filenames.items():\n",
    "    corpus[name] = dict()\n",
    "    for lang, filename in d.items():\n",
    "        with open(filename, \"r\") as f:\n",
    "            corpus[name][lang] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b0b176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 20:36:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 50.9MB/s]                    \n",
      "2026-02-09 20:36:14 INFO: Downloaded file to /home/patrik/stanza_resources/resources.json\n",
      "2026-02-09 20:36:14 INFO: Loading these models for language: en (English):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2026-02-09 20:36:14 INFO: Using device: cpu\n",
      "2026-02-09 20:36:14 INFO: Loading: tokenize\n",
      "2026-02-09 20:36:14 INFO: Loading: pos\n",
      "2026-02-09 20:36:23 INFO: Done loading processors!\n",
      "2026-02-09 20:36:23 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 436kB [00:00, 13.9MB/s]                    \n",
      "2026-02-09 20:36:23 INFO: Downloaded file to /home/patrik/stanza_resources/resources.json\n",
      "2026-02-09 20:36:25 INFO: Loading these models for language: cs (Czech):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | pdt          |\n",
      "| pos       | pdt_nocharlm |\n",
      "============================\n",
      "\n",
      "2026-02-09 20:36:25 INFO: Using device: cpu\n",
      "2026-02-09 20:36:25 INFO: Loading: tokenize\n",
      "2026-02-09 20:36:25 INFO: Loading: pos\n",
      "2026-02-09 20:36:29 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_en = stanza.Pipeline(lang='en', processors='tokenize,pos', tokenize_pretokenized=True)\n",
    "nlp_cs = stanza.Pipeline(lang='cs', processors='tokenize,pos', tokenize_pretokenized=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b094922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL FUNCTIONS - FOR DEBUGGING AND STUFF - NOT USED CURRENTLY\n",
    "\n",
    "def line_number_check(data, text):\n",
    "    i=0\n",
    "    for line in data.split(\"\\n\"):\n",
    "        splitted = line.split(\"\\t\")\n",
    "        if len(splitted) == 0 or splitted[0].startswith(\"#\") or \"-\" in splitted[0] or \".\" in splitted[0] or splitted[0] == \"\\n\" or splitted[0] == \"\":\n",
    "            continue\n",
    "        i += 1\n",
    "    print(f\"{text} size : {i}\")\n",
    "\n",
    "\n",
    "def word_tag_count(data, text):\n",
    "    word_tag_pairs = set()\n",
    "    i = 0\n",
    "    for line in data.split(\"\\n\"):\n",
    "        splitted = line.split(\"\\t\")\n",
    "        if len(splitted) == 0 or splitted[0].startswith(\"#\") or \"-\" in splitted[0] or \".\" in splitted[0] or splitted[0] == \"\\n\" or splitted[0] == \"\":\n",
    "            continue\n",
    "        word_tag_pairs.add((splitted[1], splitted[4]))\n",
    "        i+=1\n",
    "\n",
    "    print(f\"{text} size : {i}\")\n",
    "\n",
    "def word_tag_count(data, text):\n",
    "    word_tag_pairs = set()\n",
    "    i = 0\n",
    "    for line in data.split(\"\\n\"):\n",
    "        splitted = line.split(\"\\t\")\n",
    "        if len(splitted) == 0 or splitted[0].startswith(\"#\") or \"-\" in splitted[0] or \".\" in splitted[0] or splitted[0] == \"\\n\" or splitted[0] == \"\":\n",
    "            continue\n",
    "        word_tag_pairs.add((splitted[1], splitted[4]))\n",
    "        i+=1\n",
    "\n",
    "    print(f\"{text} size : {i}\")\n",
    "    # print(f\"{text} word-tag pairs : {len(word_tag_pairs)}\")\n",
    "\n",
    "# line_number_check(corpus[\"train\"][\"en\"], \"English train\")\n",
    "# line_number_check(corpus[\"dev\"][\"cs\"], \"Czech dev\")\n",
    "# line_number_check(corpus[\"test\"][\"cs\"], \"Czech test\")\n",
    "\n",
    "# word_tag_count(corpus[\"test\"][\"en\"], \"English test\")\n",
    "# word_tag_count(\"\\n\\n\".join(corpus[\"test\"][\"cs\"].split(\"\\n\\n\")[:1500]), \"Czech test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31b2028b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE TRUNCATION TO 1_500 SENTENCES\n",
      "test-cs sentence count: 20187 word/tag pairs (lines): 305808\n",
      "test-en sentence count: 1464 word/tag pairs (lines): 28397\n",
      "dev-cs sentence count: 22666 word/tag pairs (lines): 384431\n",
      "dev-en sentence count: 1575 word/tag pairs (lines): 28119\n",
      "train-cs sentence count: 12519 word/tag pairs (lines): 218409\n",
      "train-en sentence count: 10224 word/tag pairs (lines): 177410\n"
     ]
    }
   ],
   "source": [
    "def tagize(filename, text, max_sentences=999_999_999):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    sentence_count=0\n",
    "    tag_count = 0\n",
    "    with open (filename, \"r\") as f:\n",
    "        for sentence in conllu.parse_incr(f):\n",
    "            sentence_tokens = []\n",
    "            sentence_tags = []\n",
    "            if sentence_count >= max_sentences:\n",
    "                break\n",
    "            for token in sentence:\n",
    "                if isinstance(token[\"id\"], (list, tuple)) and (\".\" in token[\"id\"] or \"-\" in token[\"id\"]):\n",
    "                    continue\n",
    "                # print(token[\"form\"], token[\"upos\"])\n",
    "                tag_count += 1\n",
    "                sentence_tokens.append(token[\"form\"])\n",
    "                sentence_tags.append(token[\"upos\"])\n",
    "            \n",
    "            sentence_count += 1\n",
    "            tokens.append(sentence_tokens)\n",
    "            tags.append(sentence_tags)\n",
    "    print(text, \"sentence count:\", sentence_count, \"word/tag pairs (lines):\", tag_count)\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "print(\"BEFORE TRUNCATION TO 1_500 SENTENCES\")\n",
    "pairs = {type: {lang: tagize(filename, f\"{type}-{lang}\") for lang, filename in d.items()} for type, d in filenames.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f80386d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUNCATING TEST SETS\n",
      "test-cs sentence count: 1500 word/tag pairs (lines): 22844\n",
      "test-en sentence count: 1464 word/tag pairs (lines): 28397\n"
     ]
    }
   ],
   "source": [
    "print(\"TRUNCATING TEST SETS\")\n",
    "pairs[\"test\"] = {lang: tagize(filename, f\"test-{lang}\", max_sentences=1500) for lang, filename in filenames[\"test\"].items()}\n",
    "tokens = {type: {lang: pair[0] for lang, pair in d.items()} for type, d in pairs.items()}\n",
    "tags = {type: {lang: pair[1] for lang, pair in d.items()} for type, d in pairs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2d70054",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_en = nlp_en(tokens[\"test\"][\"en\"])\n",
    "doc_cs = nlp_cs(tokens[\"test\"][\"cs\"][:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56efcb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanza EN test accuracy: 97.8308%\n",
      "Stanza EN confusion matrix below\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>AUX</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PART</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>SYM</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>1755</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0</td>\n",
       "      <td>2833</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>1226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1520</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>987</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2515</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4806</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>478</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>668</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1677</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUNCT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3547</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2951</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred    ADJ   ADP   ADV   AUX  CCONJ   DET  INTJ  NOUN  NUM  PART  PRON  \\\n",
       "gold                                                                      \n",
       "ADJ    1755     2    11     0      0     0     0    34    0     0     0   \n",
       "ADP       0  2833    12     0      2     0     0     0    0     1     0   \n",
       "ADV      37    36  1226     0      0    14     6     8    0     0     3   \n",
       "AUX       0     0     0  1520      0     0     0     0    0     0     0   \n",
       "CCONJ     0     0     0     0    987     0     0     0    0     0     0   \n",
       "DET       0     0     1     0      3  2515     1     0    0     0     1   \n",
       "INTJ      1     4    12     0      1     0   155     2    0     0     0   \n",
       "NOUN     25     0     0     1      0     0     2  4806    3     0     1   \n",
       "NUM       0     1     0     0      0     0     0     0  478     0     0   \n",
       "PART      0     1     0     1      0     1     0     0    0   668     0   \n",
       "PRON      0     0     0     0      0     2     0     1    0     0  2178   \n",
       "PROPN    40     1     0     0      0     1     1    64    2     0     0   \n",
       "PUNCT     0     0     0     0      0     0     0     0    0     2     0   \n",
       "SCONJ     0    22     1     0      0     0     0     0    0     0     7   \n",
       "SYM       0     0     0     0      0     0     0     5    0     0     0   \n",
       "VERB     23     0     0    12      0     0     1    33    0     0     0   \n",
       "X         0     0     0     0      0     0     0     3    4     0     0   \n",
       "\n",
       "pred   PROPN  PUNCT  SCONJ  SYM  VERB   X  \n",
       "gold                                       \n",
       "ADJ       13      0      0    0    18   0  \n",
       "ADP        2      0     15    0     0   0  \n",
       "ADV        1      0      4    0     1   0  \n",
       "AUX        0      0      0    0    13   0  \n",
       "CCONJ      0      0      1    0     0   0  \n",
       "DET        0      0      0    0     0   0  \n",
       "INTJ       1      0      3    0     2   3  \n",
       "NOUN      59      0      0    0     8   1  \n",
       "NUM        0      0      0    0     0   0  \n",
       "PART       0      0      0    0     0   0  \n",
       "PRON       0      0      6    0     0   0  \n",
       "PROPN   1677      0      0    0     1   0  \n",
       "PUNCT      0   3547      0    1     0   0  \n",
       "SCONJ      0      0    438    0     0   0  \n",
       "SYM        0      2      0   29     0   0  \n",
       "VERB       0      0      0    0  2951   0  \n",
       "X          8      0      0    0     0  18  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanza CS test accuracy: 97.2903%\n",
      "Stanza CS confusion matrix below\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>AUX</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PART</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>SYM</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>2572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0</td>\n",
       "      <td>2109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>878</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>774</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>731</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>916</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5525</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>730</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUNCT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3461</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred    ADJ   ADP  ADV  AUX  CCONJ  DET  INTJ  NOUN  NUM  PART  PRON  PROPN  \\\n",
       "gold                                                                          \n",
       "ADJ    2572     0    1    0      0    0     0    16    1     0     0     11   \n",
       "ADP       0  2109    0    0      0    0     0     3    0     0     1      0   \n",
       "ADV       1     2  878    0     16    1     0    10    0     5     3      0   \n",
       "AUX       0     0    0  774      0    0     0     0    0     0     0      0   \n",
       "CCONJ     0     0   12    0    731    0     0     0    0     2     0      0   \n",
       "DET       1     0    1    0      0  916     0     0    0     0     0      0   \n",
       "INTJ      0     0    0    0      0    0    14     2    0     3     0      1   \n",
       "NOUN     12     3    7    0      4    0     4  5525    0     0     0     32   \n",
       "NUM       0     0    0    0      0    0     0     0  616     0     0      0   \n",
       "PART      0     0  230    0     79    0     2     2    0   160     0      0   \n",
       "PRON      1     6    0    4      0   14     0     4    0     0   730      0   \n",
       "PROPN     2     0    0    0      0    0     3    36    0     0     3   1026   \n",
       "PUNCT     0     0    0    0      0    0     0     0    0     0     0      0   \n",
       "SCONJ     0     0    7    0      0    0     0     0    0     2     3      0   \n",
       "SYM       0     0    0    0      0    0     0     3    0     0     0      0   \n",
       "VERB      1     0    0    0      0    0     0     4    0     0     0      4   \n",
       "X         0     0    0    0      0    0     0     3    0     0     0     17   \n",
       "\n",
       "pred   PUNCT  SCONJ  SYM  VERB   X  \n",
       "gold                                \n",
       "ADJ        0      0    0     0   1  \n",
       "ADP        0      0    0     1   0  \n",
       "ADV        0      0    0     1   0  \n",
       "AUX        0      0    0     2   0  \n",
       "CCONJ      0     11    0     0   0  \n",
       "DET        0      0    0     1   0  \n",
       "INTJ       0      0    0     0   0  \n",
       "NOUN       0      0    6     3   4  \n",
       "NUM        0      0    0     0   0  \n",
       "PART       0      2    0     0   0  \n",
       "PRON       0      0    0     0   0  \n",
       "PROPN      0      0    0     1   1  \n",
       "PUNCT   3461      0    0     0   0  \n",
       "SCONJ      0    438    0     0   0  \n",
       "SYM        0      0   38     0   0  \n",
       "VERB       0      0    0  2148   0  \n",
       "X          0      0    0     0  89  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def stanza_eval(doc, tokens_sents, gold_sents):\n",
    "    \"\"\"tokens_sents: List[List[str]], gold_sents: List[List[str]] (UPOS)\"\"\"\n",
    "    pred_sents = [[w.upos for w in s.words] for s in doc.sentences]\n",
    "\n",
    "    # accuracy\n",
    "    correct = total = 0\n",
    "    for g, p in zip(gold_sents, pred_sents):\n",
    "        if len(g) != len(p):\n",
    "            raise ValueError(f\"Length mismatch in a sentence: gold={len(g)} pred={len(p)} \"\n",
    "                             \"(check MWT/ellipsis filtering + tokenize_pretokenized=True)\")\n",
    "        for gt, pt in zip(g, p):\n",
    "            total += 1\n",
    "            correct += (gt == pt)\n",
    "    acc = 100.0 * correct / total\n",
    "\n",
    "    # confusion matrix (rows=gold, cols=pred)\n",
    "    gflat = [t for sent in gold_sents for t in sent]\n",
    "    pflat = [t for sent in pred_sents for t in sent]\n",
    "    cm = pd.crosstab(pd.Series(gflat, name=\"gold\"), pd.Series(pflat, name=\"pred\"))\n",
    "    return acc, cm\n",
    "\n",
    "en_acc, en_cm = stanza_eval(doc_en, tokens[\"test\"][\"en\"], tags[\"test\"][\"en\"])\n",
    "print(f\"Stanza EN test accuracy: {en_acc:.4f}%\")\n",
    "print(\"Stanza EN confusion matrix below\")\n",
    "display(en_cm)\n",
    "\n",
    "cs_acc, cs_cm = stanza_eval(doc_cs, tokens[\"test\"][\"cs\"][:1500], tags[\"test\"][\"cs\"][:1500])\n",
    "print(f\"Stanza CS test accuracy: {cs_acc:.4f}%\")\n",
    "print(\"Stanza CS confusion matrix below\")\n",
    "display(cs_cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36100994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUNKATING CS DEV SET\n",
      "dev-cs sentence count: 1500 word/tag pairs (lines): 21752\n"
     ]
    }
   ],
   "source": [
    "print(\"TRUNKATING CS DEV SET\")\n",
    "tokens[\"dev\"][\"cs\"], tags[\"dev\"][\"cs\"] = tagize(filenames[\"dev\"][\"cs\"], \"dev-cs\", max_sentences=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "708c4c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes check\n",
      "test-cs tokenized sentences: 1500 tokens : 22844\n",
      "test-en tokenized sentences: 1464 tokens : 28397\n",
      "dev-cs tokenized sentences: 1500 tokens : 21752\n",
      "dev-en tokenized sentences: 1575 tokens : 28119\n",
      "train-cs tokenized sentences: 12519 tokens : 218409\n",
      "train-en tokenized sentences: 10224 tokens : 177410\n"
     ]
    }
   ],
   "source": [
    "# Check sizes\n",
    "print(\"Sizes check\")\n",
    "for type, d in tokens.items():\n",
    "    for lang, sents in d.items():\n",
    "        print(f\"{type}-{lang} tokenized sentences: {len(sents)} tokens : {sum(len(s) for s in sents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf75a19",
   "metadata": {},
   "source": [
    "# Transition model smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8be7afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "\n",
    "# UPDATED BIGRAMS SO THERE IS ALSO start_symbol followed by start_symbol (and also end followed by end)\n",
    "get_unigrams = lambda tokens : Counter(tokens + [EOS])\n",
    "get_bigrams = lambda tokens : Counter(zip(2 * [BOS] + tokens + [EOS], [BOS] + tokens + 2 * [EOS]))\n",
    "get_trigrams = lambda tokens : Counter(zip(2 * [BOS] + tokens, [BOS] + tokens + [EOS], tokens + 2 * [EOS]))\n",
    "\n",
    "def get_all_ngrams(tokens_or_tags, n=1):\n",
    "    ngrams = {\n",
    "        1: get_unigrams,\n",
    "        2: get_bigrams,\n",
    "        3: get_trigrams\n",
    "    }[n]\n",
    "    all = Counter()    \n",
    "    for sentence in tokens_or_tags:\n",
    "        all.update(ngrams(sentence))\n",
    "    return all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95e8d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM THE FIRST ASSIGNMENT\n",
    "# NOT IDEA IF THIS IS CORRECT SINCE I REVIECED NO FEEDBACK :)\n",
    "# BUT I GOT 92 POINTS SO HOPEFULLY THIS IS OK\n",
    "\n",
    "def bigram_probab(bigrams, unigrams, wi, wi_1):\n",
    "    c2 = bigrams[(wi_1, wi)]#  if (wi_1, wi) in bigrams else 0\n",
    "    c1 = unigrams[wi_1]# if wi_1 in unigrams else 0\n",
    "    if c1 == 0:\n",
    "        return 1 / len(unigrams)\n",
    "    return c2 / c1\n",
    "\n",
    "def trigram_probab(trigrams, bigrams, unigrams, wi, wi_1, wi_2):\n",
    "    c3 = trigrams[(wi_2, wi_1, wi)]# if (wi_2, wi_1, wi) in trigrams else 0\n",
    "    c2 = bigrams[(wi_2, wi_1)]# if (wi_2, wi_1) in bigrams else 0\n",
    "    if c2 == 0:\n",
    "        return 1 / len(unigrams)\n",
    "    return c3 / c2\n",
    "\n",
    "def smoothed_probab_trigram(trigrams, bigrams, unigrams, wi, wi_1, wi_2, lambdas, N_uni = None):\n",
    "    N_uni = sum(unigrams.values()) if N_uni == None else N_uni\n",
    "    lambda0, lambda1, lambda2, lambda3 = lambdas\n",
    "    p0 = 1 / len(unigrams)\n",
    "    p1 = unigrams[wi] / N_uni# if wi in unigrams else 1 / sum(unigrams.values())\n",
    "    p2 = bigram_probab(bigrams, unigrams, wi, wi_1)\n",
    "    p3 = trigram_probab(trigrams, bigrams, unigrams, wi, wi_1, wi_2)\n",
    "    return lambda0 * p0 + lambda1 * p1 + lambda2 * p2 + lambda3 * p3\n",
    "\n",
    "\n",
    "def EM_smoothing(train, heldout, max_iter=100, e = 1e-4):\n",
    "    lambdas = [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "    # --- ONE MINOR CHANGE FROM THE FIRST ASSIGNMENT ---\n",
    "    # trigrams_heldout = get_trigrams(heldout)\n",
    "    # bigrams_heldout = get_bigrams(heldout)\n",
    "    # unigrams_heldout = get_unigrams(heldout)\n",
    "\n",
    "    # trigrams_train = get_trigrams(train)\n",
    "    # bigrams_train = get_bigrams(train)\n",
    "    # unigrams_train = get_unigrams(train)\n",
    "\n",
    "    trigrams_heldout = get_all_ngrams(heldout, n=3)\n",
    "    bigrams_heldout = get_all_ngrams(heldout, n=2)\n",
    "    unigrams_heldout = get_all_ngrams(heldout, n=1)\n",
    "\n",
    "    trigrams_train = get_all_ngrams(train, n=3)\n",
    "    bigrams_train = get_all_ngrams(train, n=2)\n",
    "    unigrams_train = get_all_ngrams(train, n=1)\n",
    "    # --- --- ----- ------ ---- --- ----- ---------- ---\n",
    "\n",
    "    N_uni = sum(unigrams_train.values()) # saves a lot of time lol\n",
    "\n",
    "    iter = 0\n",
    "    while iter < max_iter:\n",
    "        counts = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "        for (wi_2, wi_1, wi), c in trigrams_heldout.items():\n",
    "            p0 = 1 / len(unigrams_train)\n",
    "            p1 = unigrams_train[wi] / N_uni# if wi in unigrams_train else 1 / N_uni\n",
    "            p2 = bigram_probab(bigrams_train, unigrams_train, wi, wi_1)\n",
    "            p3 = trigram_probab(trigrams_train, bigrams_train, unigrams_train, wi, wi_1, wi_2)\n",
    "\n",
    "            p_total = lambdas[0] * p0 + lambdas[1] * p1 + lambdas[2] * p2 + lambdas[3] * p3\n",
    "\n",
    "            counts[0] += c * (lambdas[0] * p0) / p_total\n",
    "            counts[1] += c * (lambdas[1] * p1) / p_total\n",
    "            counts[2] += c * (lambdas[2] * p2) / p_total\n",
    "            counts[3] += c * (lambdas[3] * p3) / p_total\n",
    "\n",
    "        new_lambdas = [count / sum(counts) for count in counts]\n",
    "\n",
    "        # print(\"Iteration \", iter, \" lambdas: \", new_lambdas)\n",
    "\n",
    "        if all(abs(new_lambdas[i] - lambdas[i]) < e for i in range(4)):\n",
    "            break\n",
    "\n",
    "        lambdas = new_lambdas\n",
    "        iter += 1\n",
    "    return lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b85b587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_train_tag = {type: {i: {lang: get_all_ngrams(tags[\"train\"][lang], n=i) for lang in tags[\"train\"]} for i in range(1, 4)} for type in tags[\"train\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cb73bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_lambdas = {lang: EM_smoothing(tags[\"train\"][lang], tags[\"dev\"][lang]) for lang in tags[\"train\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63155b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs': [0.005635399093115278,\n",
       "  0.005527962553778706,\n",
       "  0.10105815981569118,\n",
       "  0.8877784785374149],\n",
       " 'en': [0.00012675633014343153,\n",
       "  0.0033791000786512265,\n",
       "  0.08104221881633325,\n",
       "  0.915451924774872]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce9f70d",
   "metadata": {},
   "source": [
    "# Emission model smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acf9af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "def train_lexical_model(tokens_train, tags_train, tokens_dev, tags_dev,\n",
    "                        unk_threshold=1, add_alpha=0.0, eps=1e-6, max_iter=80):\n",
    "    \"\"\"\n",
    "    Learns interpolated emission model with UNK:\n",
    "\n",
    "      P(w|t) = λ0 * 1/|V|  +  λ1 * P(w)  +  λ2 * P_MLE(w|t)\n",
    "\n",
    "    - UNK: words with train freq <= unk_threshold become <UNK> in training,\n",
    "           unseen words at decode time also become <UNK>.\n",
    "    - Lambdas (λ0,λ1,λ2) learned on DEV with EM-like expected-counts updates.\n",
    "\n",
    "    Returns dict with:\n",
    "      - logp_emit(tag, word): log P(word|tag)\n",
    "      - lambdas: [λ0, λ1, λ2]\n",
    "      - vocab: normalized vocab (includes <UNK>)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- TRAIN vocab + UNK mapping ---\n",
    "    wf = Counter(w for sent in tokens_train for w in sent)\n",
    "\n",
    "    def norm_train(w):\n",
    "        return w if wf[w] > unk_threshold else UNK\n",
    "\n",
    "    emit = Counter()         # (t, w)\n",
    "    tag_count = Counter()    # t\n",
    "    word_count = Counter()   # w\n",
    "    vocab = {UNK}\n",
    "    total = 0\n",
    "\n",
    "    for words, tags in zip(tokens_train, tags_train):\n",
    "        for w, t in zip(words, tags):\n",
    "            w2 = norm_train(w)\n",
    "            vocab.add(w2)\n",
    "            emit[(t, w2)] += 1\n",
    "            tag_count[t] += 1\n",
    "            word_count[w2] += 1\n",
    "            total += 1\n",
    "\n",
    "    V = len(vocab)\n",
    "\n",
    "    def p0(w):  # uniform over vocab\n",
    "        return 1.0 / V\n",
    "\n",
    "    def p1(w):  # unigram P(w)\n",
    "        denom = total + add_alpha * V\n",
    "        return (word_count[w] + add_alpha) / denom if denom else 0.0\n",
    "\n",
    "    def p2(t, w):  # MLE P(w|t)\n",
    "        denom = tag_count[t] + add_alpha * V\n",
    "        return (emit[(t, w)] + add_alpha) / denom if denom else 0.0\n",
    "\n",
    "    def norm_dev(w):\n",
    "        # unseen -> UNK\n",
    "        return w if w in vocab else UNK\n",
    "\n",
    "    # --- learn lambdas on DEV ---\n",
    "    lambdas = [1/3, 1/3, 1/3]\n",
    "    for _ in range(max_iter):\n",
    "        exp = [0.0, 0.0, 0.0]\n",
    "\n",
    "        for words, tags in zip(tokens_dev, tags_dev):\n",
    "            for w_raw, t in zip(words, tags):\n",
    "                w = norm_dev(w_raw)\n",
    "                probs = [p0(w), p1(w), p2(t, w)]\n",
    "                mix = [lambdas[i] * probs[i] for i in range(3)]\n",
    "                Z = sum(mix)\n",
    "                if Z <= 0:\n",
    "                    continue\n",
    "                exp[0] += mix[0] / Z\n",
    "                exp[1] += mix[1] / Z\n",
    "                exp[2] += mix[2] / Z\n",
    "\n",
    "        s = sum(exp)\n",
    "        new_lambdas = [e / s for e in exp] if s else lambdas\n",
    "        if max(abs(new_lambdas[i] - lambdas[i]) for i in range(3)) < eps:\n",
    "            lambdas = new_lambdas\n",
    "            break\n",
    "        lambdas = new_lambdas\n",
    "\n",
    "    def logp_emit(t, w_raw):\n",
    "        w = w_raw if w_raw in vocab else UNK\n",
    "        p = lambdas[0] * p0(w) + lambdas[1] * p1(w) + lambdas[2] * p2(t, w)\n",
    "        return -math.inf if p <= 0.0 else math.log(p)\n",
    "\n",
    "    return {\"vocab\": vocab, \"lambdas\": lambdas, \"logp_emit\": logp_emit,\n",
    "            \"emit\": emit, \"tag_count\": tag_count, \"word_count\": word_count}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02ba48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = {}\n",
    "for lang in [\"en\", \"cs\"]:\n",
    "    lex[lang] = train_lexical_model(\n",
    "        tokens[\"train\"][lang], tags[\"train\"][lang],\n",
    "        tokens[\"dev\"][lang],   tags[\"dev\"][lang],\n",
    "        unk_threshold=1, add_alpha=0.0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b387a7b",
   "metadata": {},
   "source": [
    "# HMM/Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24f678a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_trigram(words, tagset, logp_trans, logp_emit, BOS=\"<s>\", EOS=\"</s>\"):\n",
    "    \"\"\"\n",
    "    Trigram Viterbi for tags t1..tn given words w1..wn.\n",
    "\n",
    "    logp_trans(u,v,t) = log P(t | u,v)\n",
    "    logp_emit(t,w)    = log P(w | t)\n",
    "\n",
    "    Returns: list of predicted tags length n\n",
    "    \"\"\"\n",
    "    n = len(words)\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    tagset = [t for t in tagset if t not in (BOS, EOS)]\n",
    "\n",
    "    # dp_i[(u,v)] = best score for sequence ending with ... u,v at position i (i>=0)\n",
    "    dp = {(BOS, BOS): 0.0}\n",
    "    bp = []  # bp[i][(u,v)] = best prevprev tag (the tag before u)\n",
    "\n",
    "    for i, w in enumerate(words, start=1):\n",
    "        new_dp = {}\n",
    "        new_bp = {}\n",
    "\n",
    "        for (pp, p), score in dp.items():\n",
    "            if score == -math.inf:\n",
    "                continue\n",
    "            for c in tagset:\n",
    "                s = score + logp_trans(pp, p, c) + logp_emit(c, w)\n",
    "                key = (p, c)\n",
    "                if s > new_dp.get(key, -math.inf):\n",
    "                    new_dp[key] = s\n",
    "                    new_bp[key] = pp  # store best prevprev for this (p,c)\n",
    "\n",
    "        dp = new_dp\n",
    "        bp.append(new_bp)\n",
    "\n",
    "    # terminate with two EOS transitions\n",
    "    best_key = None\n",
    "    best_score = -math.inf\n",
    "    for (u, v), score in dp.items():\n",
    "        s_end = score + logp_trans(u, v, EOS) + logp_trans(v, EOS, EOS)\n",
    "        if s_end > best_score:\n",
    "            best_score = s_end\n",
    "            best_key = (u, v)\n",
    "\n",
    "    # backtrack\n",
    "    tags_out = [None] * n\n",
    "    u, v = best_key\n",
    "    tags_out[n-2] = u if n >= 2 else v\n",
    "    tags_out[n-1] = v\n",
    "\n",
    "    # i indexes 1..n, bp index is i-1\n",
    "    # For i=n: we know (t_{n-1}, t_n) = (u,v) and bp[n-1][(u,v)] gives t_{n-2}\n",
    "    for i in range(n, 2, -1):\n",
    "        pp = bp[i-1][(tags_out[i-2], tags_out[i-1])]\n",
    "        tags_out[i-3] = pp\n",
    "\n",
    "    # For n=1, tags_out is [v] already correct; for n=2, loop doesn’t run; ok.\n",
    "    if n == 1:\n",
    "        tags_out = [v]\n",
    "\n",
    "    return tags_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c201eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_first_n_pairs(tokens_sents, tags_sents, n_pairs):\n",
    "    out_tok, out_tag = [], []\n",
    "    seen = 0\n",
    "    for words, tags in zip(tokens_sents, tags_sents):\n",
    "        if seen >= n_pairs:\n",
    "            break\n",
    "        need = n_pairs - seen\n",
    "        if len(words) <= need:\n",
    "            out_tok.append(words)\n",
    "            out_tag.append(tags)\n",
    "            seen += len(words)\n",
    "        else:\n",
    "            out_tok.append(words[:need])\n",
    "            out_tag.append(tags[:need])\n",
    "            seen += need\n",
    "            break\n",
    "    return out_tok, out_tag\n",
    "\n",
    "def build_logp_trans_from_train_tags(lang, tags_train_sents):\n",
    "    uni = get_all_ngrams(tags_train_sents, n=1)\n",
    "    bi  = get_all_ngrams(tags_train_sents, n=2)\n",
    "    tri = get_all_ngrams(tags_train_sents, n=3)\n",
    "    N_uni = sum(uni.values())\n",
    "    lambdas = em_lambdas[lang]\n",
    "\n",
    "    def logp_trans(u, v, t):\n",
    "        p = smoothed_probab_trigram(tri, bi, uni, wi=t, wi_1=v, wi_2=u, lambdas=lambdas, N_uni=N_uni)\n",
    "        return -math.inf if p <= 0.0 else math.log(p)\n",
    "\n",
    "    tagset = [t for t in uni.keys() if t not in (BOS, EOS)]\n",
    "    return logp_trans, tagset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def eval_tagging(gold_sents, pred_sents):\n",
    "    gold_flat, pred_flat = [], []\n",
    "    for i, (g, p) in enumerate(zip(gold_sents, pred_sents)):\n",
    "        if len(g) != len(p):\n",
    "            raise ValueError(f\"Sentence {i} length mismatch: gold={len(g)} pred={len(p)}\")\n",
    "        gold_flat.extend(g)\n",
    "        pred_flat.extend(p)\n",
    "    acc = 100.0 * sum(gt == pt for gt, pt in zip(gold_flat, pred_flat)) / len(gold_flat)\n",
    "    cm = pd.crosstab(pd.Series(gold_flat, name=\"gold\"), pd.Series(pred_flat, name=\"pred\"))\n",
    "    return acc, cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0ed6c58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens used: 10000\n",
      "Train tokens used: 10000\n"
     ]
    }
   ],
   "source": [
    "def take_first_n_tokens(tokens_sents, tags_sents, n_tokens):\n",
    "    \"\"\"\n",
    "    Keeps sentence structure but truncates after n_tokens total tokens.\n",
    "    \"\"\"\n",
    "    out_tokens, out_tags = [], []\n",
    "    count = 0\n",
    "\n",
    "    for words, tags in zip(tokens_sents, tags_sents):\n",
    "        if count >= n_tokens:\n",
    "            break\n",
    "\n",
    "        if count + len(words) <= n_tokens:\n",
    "            out_tokens.append(words)\n",
    "            out_tags.append(tags)\n",
    "            count += len(words)\n",
    "        else:\n",
    "            # partial sentence\n",
    "            k = n_tokens - count\n",
    "            out_tokens.append(words[:k])\n",
    "            out_tags.append(tags[:k])\n",
    "            count = n_tokens\n",
    "            break\n",
    "\n",
    "    return out_tokens, out_tags\n",
    "\n",
    "tokens_train_10k_en, tags_train_10k_en = take_first_n_tokens(\n",
    "    tokens[\"train\"][\"en\"],\n",
    "    tags[\"train\"][\"en\"],\n",
    "    10_000\n",
    ")\n",
    "\n",
    "tokens_train_10k_cs, tags_train_10k_cs = take_first_n_tokens(\n",
    "    tokens[\"train\"][\"cs\"],\n",
    "    tags[\"train\"][\"cs\"],\n",
    "    10_000\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Train tokens used:\", sum(len(s) for s in tokens_train_10k_en))\n",
    "print(\"Train tokens used:\", sum(len(s) for s in tokens_train_10k_cs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9deff467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag n-grams from reduced training set\n",
    "uni_10k_en = get_all_ngrams(tags_train_10k_en, n=1)\n",
    "bi_10k_en  = get_all_ngrams(tags_train_10k_en, n=2)\n",
    "tri_10k_en = get_all_ngrams(tags_train_10k_en, n=3)\n",
    "\n",
    "uni_10k_cs = get_all_ngrams(tags_train_10k_cs, n=1)\n",
    "bi_10k_cs  = get_all_ngrams(tags_train_10k_cs, n=2)\n",
    "tri_10k_cs = get_all_ngrams(tags_train_10k_cs, n=3)\n",
    "\n",
    "# Lambdas still estimated on FULL dev set (important!)\n",
    "lambdas_10k_en = em_lambdas[\"en\"]\n",
    "lambdas_10k_cs = em_lambdas[\"cs\"]\n",
    "\n",
    "N_uni_10k_en = sum(uni_10k_en.values())\n",
    "N_uni_10k_cs = sum(uni_10k_cs.values())\n",
    "\n",
    "lex_10k_en = train_lexical_model(\n",
    "    tokens_train_10k_en,\n",
    "    tags_train_10k_en,\n",
    "    unk_threshold=1,\n",
    "    add_alpha=0.0\n",
    ")\n",
    "\n",
    "lex_10k_cs = train_lexical_model(\n",
    "    tokens_train_10k_cs,\n",
    "    tags_train_10k_cs,\n",
    "    unk_threshold=1,\n",
    "    add_alpha=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e599075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_trans_10k_en(u, v, t):\n",
    "    p = smoothed_probab_trigram(\n",
    "        tri_10k_en, bi_10k_en, uni_10k_en,\n",
    "        wi=t, wi_1=v, wi_2=u,\n",
    "        lambdas=lambdas_10k_en,\n",
    "        N_uni=N_uni_10k_en\n",
    "    )\n",
    "    return -math.inf if p <= 0 else math.log(p)\n",
    "\n",
    "def logp_trans_10k_cs(u, v, t):\n",
    "    p = smoothed_probab_trigram(\n",
    "        tri_10k_cs, bi_10k_cs, uni_10k_cs,\n",
    "        wi=t, wi_1=v, wi_2=u,\n",
    "        lambdas=lambdas_10k_cs,\n",
    "        N_uni=N_uni_10k_cs\n",
    "    )\n",
    "    return -math.inf if p <= 0 else math.log(p)\n",
    "\n",
    "\n",
    "\n",
    "tagset_10k_en = [t for t in uni_10k_en if t not in (BOS, EOS)]\n",
    "tagset_10k_cs = [t for t in uni_10k_cs if t not in (BOS, EOS)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ba6ae582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised HMM (init, 10k supervised) EN accuracy: 72.4337\n",
      "Unsupervised HMM (init, 10k supervised) EN confusion matrix below\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>AUX</th>\n",
       "      <th>CCONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PART</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>SYM</th>\n",
       "      <th>VERB</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>1094</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>231</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>220</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>16</td>\n",
       "      <td>2581</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>95</td>\n",
       "      <td>64</td>\n",
       "      <td>688</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>907</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2400</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>391</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3534</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>565</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>303</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>507</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>156</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1144</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>149</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>234</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>665</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>687</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>99</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PUNCT</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3512</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>8</td>\n",
       "      <td>131</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>205</td>\n",
       "      <td>16</td>\n",
       "      <td>74</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>499</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1806</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred    ADJ   ADP  ADV   AUX  CCONJ   DET  INTJ  NOUN  NUM  PART  PRON  PROPN  \\\n",
       "gold                                                                            \n",
       "ADJ    1094     4   52     0      0     0     0   231    7     0     1    213   \n",
       "ADP      16  2581   24     0      2     2     0    24    0    68     0     32   \n",
       "ADV      95    64  688     0      1    19     0   177    7     0    36     95   \n",
       "AUX      21     0    8  1174      0     0     0    39    2    79     3     73   \n",
       "CCONJ    10     0   39     0    907     1     0     4    0     0     5      8   \n",
       "DET      15     0    3     0     10  2400     0    11    0     0    38     23   \n",
       "INTJ     21     8   78     0      0     0     2    21    3     0     1     32   \n",
       "NOUN    391     0   54     2      0     1     0  3534   27     0     5    565   \n",
       "NUM      37     0    4     0      0     0     0    76  239     1    22     77   \n",
       "PART      6    56   35     0      0     1     0    12    0   507     0     25   \n",
       "PRON    156     3   96     0      1    18     0   298   12     5  1144    204   \n",
       "PROPN   234     3   45     0      2     0     0   665   22     0     3    687   \n",
       "PUNCT     1     0    1     0      0     0     0    11    0     2     0     11   \n",
       "SCONJ     8   131   12     0      4     1     0     5    0     4    19      4   \n",
       "SYM       2     1    0     0      1     0     0     2    1     0     0      2   \n",
       "VERB    205    16   74    94      1     0     0   499    6    10     5    286   \n",
       "X         3     1    1     0      0     0     0     3    1     0     0     10   \n",
       "\n",
       "pred   PUNCT  SCONJ  SYM  VERB   X  \n",
       "gold                                \n",
       "ADJ        0      0    2   220   9  \n",
       "ADP        0     48    0    62   6  \n",
       "ADV        0     16    2   132   4  \n",
       "AUX        0      0    0   132   2  \n",
       "CCONJ      0      1    1    10   2  \n",
       "DET        0     12    0     5   4  \n",
       "INTJ       0      2    1    10   5  \n",
       "NOUN       0      1    4   303  19  \n",
       "NUM        0      0    0    20   3  \n",
       "PART       1      0    1    27   0  \n",
       "PRON       0     84    3   149  14  \n",
       "PROPN      0      2    3    99  22  \n",
       "PUNCT   3512      0    7     4   1  \n",
       "SCONJ      0    264    0    15   1  \n",
       "SYM        6      0   18     3   0  \n",
       "VERB       0      4    0  1806  14  \n",
       "X          0      0    0     2  12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_sents_10k_en = [\n",
    "    viterbi_trigram(sent, tagset_10k_en, logp_trans_10k_en, lex_10k_en[\"logp_emit\"])\n",
    "    for sent in tokens[\"test\"][\"en\"]\n",
    "]\n",
    "\n",
    "gold_flat, pred_flat = [], []\n",
    "for g, p in zip(tags[\"test\"][\"en\"], pred_sents_10k_en):\n",
    "    if len(g) != len(p):\n",
    "        raise ValueError(\"Length mismatch during evaluation\")\n",
    "    gold_flat.extend(g)\n",
    "    pred_flat.extend(p)\n",
    "\n",
    "acc_10k_en = 100.0 * sum(gt == pt for gt, pt in zip(gold_flat, pred_flat)) / len(gold_flat)\n",
    "print(f\"Unsupervised HMM (init, 10k supervised) EN accuracy: {acc_10k_en:.4f}\")\n",
    "\n",
    "print(\"Unsupervised HMM (init, 10k supervised) EN confusion matrix below\")\n",
    "cm_10k_en = pd.crosstab(\n",
    "    pd.Series(gold_flat, name=\"gold\"),\n",
    "    pd.Series(pred_flat, name=\"pred\")\n",
    ")\n",
    "\n",
    "display(cm_10k_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0f26f504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised HMM (init, 10k supervised) CS accuracy: 68.5169\n",
      "Unsupervised HMM (init, 10k supervised) CS confusion matrix below\n"
     ]
    }
   ],
   "source": [
    "pred_sents_10k_cs = [\n",
    "    viterbi_trigram(sent, tagset_10k_cs, logp_trans_10k_cs, lex_10k_cs[\"logp_emit\"])\n",
    "    for sent in tokens[\"test\"][\"cs\"]\n",
    "]\n",
    "\n",
    "gold_flat, pred_flat = [], []\n",
    "for g, p in zip(tags[\"test\"][\"cs\"], pred_sents_10k_cs):\n",
    "    if len(g) != len(p):\n",
    "        raise ValueError(\"Length mismatch during evaluation\")\n",
    "    gold_flat.extend(g)\n",
    "    pred_flat.extend(p)\n",
    "\n",
    "acc_10k_cs = 100.0 * sum(gt == pt for gt, pt in zip(gold_flat, pred_flat)) / len(gold_flat)\n",
    "print(f\"Unsupervised HMM (init, 10k supervised) CS accuracy: {acc_10k_cs:.4f}\")\n",
    "\n",
    "print(\"Unsupervised HMM (init, 10k supervised) CS confusion matrix below\")\n",
    "cm_10k_cs = pd.crosstab(\n",
    "    pd.Series(gold_flat, name=\"gold\"),\n",
    "    pd.Series(pred_flat, name=\"pred\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
